---
title: "Memory"
icon: memory
---

For more detailed usage information, please refer to our cookbook: [Memory Cookbook](../cookbooks/advanced_features/agents_with_memory.ipynb)

<Card title="What is Memory?" icon="database">
  CAMEL's memory system helps agents recall relevant past information, making conversations context-aware and responses more coherent.
</Card>

## Getting Started

<Steps>
  <Step title="Basic Setup">
  
  Here's a simple example of how to use `LongtermAgentMemory`:

  ```python
  from camel.memories import (
      ChatHistoryBlock,
      LongtermAgentMemory,
      MemoryRecord,
      ScoreBasedContextCreator,
      VectorDBBlock,
  )
  from camel.messages import BaseMessage
  from camel.types import ModelType, OpenAIBackendRole
  from camel.utils import OpenAITokenCounter

  memory = LongtermAgentMemory(
      context_creator=ScoreBasedContextCreator(
          token_counter=OpenAITokenCounter(ModelType.GPT_4O_MINI),
          token_limit=1024,
      ),
      chat_history_block=ChatHistoryBlock(),
      vector_db_block=VectorDBBlock(),
  )

  records = [
      MemoryRecord(
          message=BaseMessage.make_user_message(
              role_name="User",
              meta_dict=None,
              content="What is CAMEL AI?",
          ),
          role_at_backend=OpenAIBackendRole.USER,
      ),
      MemoryRecord(
          message=BaseMessage.make_assistant_message(
              role_name="Agent",
              meta_dict=None,
              content="CAMEL-AI.org is the 1st LLM multi-agent framework and "
                      "an open-source community dedicated to finding the scaling law "
                      "of agents.",
          ),
          role_at_backend=OpenAIBackendRole.ASSISTANT,
      ),
  ]
  memory.write_records(records)

  context, token_count = memory.get_context()
  print(f"Retrieved context (token count: {token_count}):")
  for msg in context:
      print(msg)
  ```

  </Step>

  <Step title="Attach Memory to Agent">
  
  Link your memory module to a `ChatAgent`:

  ```python
  from camel.agents import ChatAgent

  sys_msg = BaseMessage.make_assistant_message(
      role_name='Agent',
      content='You are a curious agent wondering about the universe.',
  )

  agent = ChatAgent(system_message=sys_msg)
  agent.memory = memory

  usr_msg = BaseMessage.make_user_message(
      role_name='User',
      content="Tell me which is the 1st LLM multi-agent framework based on what we discussed?",
  )

  response = agent.step(usr_msg)
  print(response.msgs[0].content)
  ```

  </Step>
</Steps>

## Core Components

<Card title="MemoryRecord" icon="bookmark">
  <b>MemoryRecord</b> is the fundamental unit of memory in CAMEL, capturing a single message and its metadata.

  **Attributes:**
  - <code>message</code>: Content of the message (type: <code>BaseMessage</code>)
  - <code>role_at_backend</code>: Role played at backend (type: <code>OpenAIBackendRole</code>)
  - <code>uuid</code>: Unique identifier for the memory
  - <code>extra_info</code>: Optional metadata

  **Methods:**
  - <code>from_dict()</code>: Build from dictionary
  - <code>to_dict()</code>: Convert to dictionary
  - <code>to_openai_message()</code>: Convert to OpenAI-compatible message format
</Card>

<Card title="ContextRecord" icon="folder">
  <b>ContextRecord</b> is the result of a memory retrieval operation, typically used by context creators to build prompts.

  **Attributes:**
  - <code>memory_record</code>: A wrapped <code>MemoryRecord</code>
  - <code>score</code>: Float score measuring relevance
</Card>

<Card title="MemoryBlock (Abstract Base Class)" icon="box">
  <b>MemoryBlock</b> is an abstract interface for all memory storage modules. It follows the Composite Design Pattern for flexibility.

  **Key Methods:**
  - <code>write_records()</code>: Add multiple records
  - <code>write_record()</code>: Add a single record
  - <code>clear()</code>: Wipe all stored memory
</Card>

<Card title="BaseContextCreator (Abstract Base Class)" icon="filter">
  <b>BaseContextCreator</b> defines strategies for constructing context windows when memory exceeds the model's token limit.

  **Attributes:**
  - <code>token_counter</code>: Logic to count message tokens
  - <code>token_limit</code>: Cap on context size

  **Method:**
  - <code>create_context(records)</code>: Generates the appropriate context block from retrieved records
</Card>

<Card title="AgentMemory (Abstract Base Class)" icon="head-side-brain">
  <b>AgentMemory</b> is an abstract memory class built to plug directly into an agent. It wraps around one or more <code>MemoryBlock</code>s and exposes context retrieval.

  **Core Methods:**
  - <code>retrieve()</code>: Pulls context records from memory
  - <code>get_context()</code>: Produces a usable conversation context within token limits
  - <code>get_context_creator()</code>: Returns the associated context creator
</Card>

## Agent Memory Implementations

<Card title="ChatHistoryMemory" icon="cloud">
  Designed for linear conversations, this memory wraps around <code>ChatHistoryBlock</code> and stores messages sequentially.

  Ideal when you want the agent to remember the last few exchanges with precise control over how much history to keep.

  <b>Configuration:</b>
  - <code>context_creator</code>: Chooses what to include in the final context
  - <code>window_size</code>: Optional number of recent messages to retrieve
  - <code>storage</code>: Optional key-value store (e.g. in-memory or persistent)
</Card>

<Card title="VectorDBMemory" icon="database">
  For deeper, more semantic memory retrieval, this one wraps <code>VectorDBBlock</code>. It transforms every memory into an embedding and retrieves via similarity search.

  Use this when your agent needs to remember what was said, not just when it was said.

  <b>Highlights:</b>
  - Retrieves contextually relevant messages from large corpora
  - Useful in exploratory or knowledge-intensive dialogues
  - Can combine with various vector stores and embedding models
</Card>

<Card title="LongtermAgentMemory" icon="memo-circle-check">
  This memory blends both recent context and semantic relevance. It pulls from both <code>ChatHistoryMemory</code> and <code>VectorDBMemory</code>, creating a more holistic view of past interactions.

  The agent gets a balanced perspective: recent events + important older info.

  <b>Why use it:</b>
  - Great for longer sessions with topic shifts
  - Fallbacks to vector memory when history limit is hit
  - Modular and flexible for custom strategies
</Card>

<Card title="Mem0 Cloud Memory" icon="cloud-arrow-up">
  Integrates <a href="https://mem0.ai/" target="_blank">Mem0</a>, a cloud-based storage layer for chat memory. Plug it directly into <code>ChatHistoryMemory</code> for persistent memory across sessions and devices.

  <b>Config options:</b>
  - <code>api_key</code>: For authenticating your account
  - <code>agent_id</code>: A unique ID to scope memory to an agent
  - <code>user_id</code>: Optional field to group memories per user
  - <code>metadata</code>: Key-value tags for tracking or filtering

  <i>This option is best if you want persistence, searchability, and a scalable memory backend without managing infrastructure.</i>
</Card>

```python
from camel.memories import ChatHistoryMemory, ScoreBasedContextCreator
from camel.storages import Mem0Storage
from camel.types import ModelType
from camel.utils import OpenAITokenCounter

memory = ChatHistoryMemory(
    context_creator=ScoreBasedContextCreator(
        token_counter=OpenAITokenCounter(ModelType.GPT_4O_MINI),
        token_limit=1024,
    ),
    storage=Mem0Storage(
        api_key="your_mem0_api_key",
        agent_id="agent123"
    ),
    agent_id="agent123"
)
```

```python
# Example usage
memory.write_records(records)
context, token_count = memory.get_context()
print(f"Token count: {token_count}")
for message in context:
    print(message)
```

```markdown
>>> Retrieved context (token count: 49):
{'role': 'user', 'content': 'What is CAMEL AI?'}
{'role': 'assistant', 'content': 'CAMEL-AI.org is the 1st LLM multi-agent framework and an open-source community dedicated to finding the scaling law of agents.'}
```

<Note>
Mem0 is perfect for stateless deployments where you need memory continuity over time. It offers automatic sync and peace of mind—ideal for agents that evolve with user behavior.
</Note>

## Advanced Topics

<Card title="Custom Context Creators" icon="sparkles">
  Sometimes, default context builders aren’t enough—especially when your use case involves non-linear histories, custom token strategies, or novel scoring logic.

  You can subclass <code>BaseContextCreator</code> to implement your own rules for building context windows. This gives you full control over what your agent sees, when, and how much.

  Here's a basic scaffold:

  ```python
  from camel.memories import BaseContextCreator

  class MyCustomContextCreator(BaseContextCreator):
      @property
      def token_counter(self):
          # Define how to count tokens
          return

      @property
      def token_limit(self):
          return 1000

      def create_context(self, records):
          # Your logic to filter, score, or truncate records
          pass
  ```
</Card>

<Card title="Embedding + Vector Store Customization" icon="database">
  <code>VectorDBBlock</code> gives you flexibility to swap embeddings and vector storage backends. Whether you're running OpenAI, HuggingFace, or custom embeddings—it all plugs in cleanly.

  Here's how you'd set it up with <code>OpenAIEmbedding</code> and <code>QdrantStorage</code>:

  ```python
  from camel.embeddings import OpenAIEmbedding
  from camel.memories import VectorDBBlock
  from camel.storages import QdrantStorage

  vector_db = VectorDBBlock(
      embedding=OpenAIEmbedding(),
      storage=QdrantStorage(vector_dim=OpenAIEmbedding().get_output_dim()),
  )
  ```

  You can easily swap in other vector DBs like Pinecone, Weaviate, or even a local FAISS backend.
</Card>

<Card title="Performance Tips" icon="gauge">
  Speed and scale matter—especially when agents run long sessions or multiple conversations in parallel.

  Some tips to keep things fast and lean:

  - Prefer persistent storages (e.g. Qdrant, Redis) over in-memory for production setups
  - Tune token limits based on expected prompt size and model context length
  - Use hybrid memory like <code>LongtermAgentMemory</code> when recency + relevance both matter
  - Regularly prune or embed older messages to avoid memory bloat
  - For fast inference, consider compact embeddings and efficient vector search (e.g. SGLang over vLLM for speed)

  Keep an eye on both retrieval latency and token overhead. A slow memory kills flow.
</Card>
